The general idea of this work was to learn about computer vision basics, test SVM as classifier for images and check any possible differences between the linear version and the kernelized versions of this algorithm.

From preprocessing part we learn how an Histogram of Gradient works and and translate an image in a vector of numerical information. We tried to investigate more about the connection between parameters and accuracy, how they can affect, in a better or worst way my classification task. Even if not a state of art method to encode images information we are quite satisfied of the results obtained with this descriptor.

From modelling part we played around to get some interesting conclusion about how kernelized version would act against a normal support vector machine. We didn't find any clear proof to state that a RBF or polynomial version of SVM perform better than a linear kernel (especially when number of instances is way smaller than the number of features) even if there are works that proof the contrary.\cite{hsu2003practical}
We than wanted to compare the SVM as classifier against other algorithm. We expected SVM to be a very good classifier in images classification and actually we got clear results on this topic. 

However we got an idea on why we didn't find any concrete results on RBF kernel on SVM. We know that RBF kernel help us distinguish datapoints when the number of feature is not enough to build a linear classifier ( in this case an hyperplane ) to classify them correctly. For that reason, applying RBF and mapping the datapoints in an higher feature space, a linear classifier can perform better. However in this case the number of feature we are building may be larger enough to allow linear SVM to classify them correctly; also looking at the images per-se we notice that they are "easy" recognizable ( same position in the picture, same profile, same background, etc ) making the pictures so different between categories that even just a linear SVM is able to do a good job.